# 一、CLIP

https://arxiv.org/pdf/2103.00020.pdf

##### Abstract

##### 1.Introduction and Motivating Work

![image-20231212111548963](C:\Users\SHENZHEBEI\AppData\Roaming\Typora\typora-user-images\image-20231212111548963.png)

对比学习：

1. 正样本：对角线

2. 负样本：非对角线元素

无需提前确认标签列表

![image-20231212214206996](C:\Users\SHENZHEBEI\AppData\Roaming\Typora\typora-user-images\image-20231212214206996.png)

采用对比学习的方式大幅度提升了效率。

##### 2.Approach

![image-20231212211537817](C:\Users\SHENZHEBEI\AppData\Roaming\Typora\typora-user-images\image-20231212211537817.png)

##### 3.Experiment

存在多义性：crane(起重机，丹顶鹤)

Distribution Gap：是指在机器学习和数据科学中，训练数据和测试数据之间的概率分布不同或存在差异的情况。当训练数据和测试数据的分布差异很大时，可能会导致模型在实际应用中表现不佳。

使用提示词：如A photo of {}。

采用80个提示词模板：

![image-20231212213905534](C:\Users\SHENZHEBEI\AppData\Roaming\Typora\typora-user-images\image-20231212213905534.png)

![image-20231212214237698](C:\Users\SHENZHEBEI\AppData\Roaming\Typora\typora-user-images\image-20231212214237698.png)

展示了在27个数据集上的结果，其中16个结果超过了原先有监督训练上的结果（在对物体分类的数据集上结果较好）。

![image-20231212214506021](C:\Users\SHENZHEBEI\AppData\Roaming\Typora\typora-user-images\image-20231212214506021.png)



4.

out of distribution:在模型训练过程中未曾见过或者不属于训练数据分布的样本。

MNIST:未被4亿个训练包含。

# 二、Vilt

# 三、GLIP

### 1. [CVPR 2022] Grounded Language-Image Pre-training 

https://arxiv.org/abs/2112.03857 

##### Abstract

将detection转化为grounding任务,采用了self-training的训练方式.

##### Introduction

### 2. [NeurIPS 2022] Unifying Localization and Vision-Language Understanding

https://arxiv.org/abs/2206.05836

### 3. [NeurIPS 2022] Flamingo: a Visual Language Model for Few-Shot Learning 

https://arxiv.org/abs/2204.14198

# 四、BLIP

双塔结构

ViLT (2021) 多模态大模型的encoder-decoder架构（单塔或双塔）

### 1. [ICML 2022]BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation

https://arxiv.org/pdf/2201.12086

### 2. [CVPR 2023]BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models

https://arxiv.org/pdf/2301.12597

# 五、2023最新论文

### 1. MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models

### 2. Visual Instruction Tuning (LLaVA)

### 3. mPLUG-Owl : Modularization Empowers Large Language Models with Multimodality

以上三个工作都有对应的v2版本。对于v2版本看看相应论文在数据、模型上的一些改变

多模态大模型综述：
相关工作，https://arxiv.org/pdf/2306.13549.pdf （https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models）
幻觉相关（最近比较火，在了解多模态大模型后关注一下，还没有一个比较好和完整的survey，可以根据hallucination进行检索一些文章）

# 六、下个工作相关的最新研究

### 1. Localized Symbolic Knowledge Distillation for Visual Commonsense Models （NeurIPS 2023）

### 2. ShareGPT4V: Improving Large Multi-Modal Models with Better Captions （2023.11）